{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99XXW-lrUQR6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99XXW-lrUQR6",
    "outputId": "aae5e6dc-04d2-4273-e8fc-d53af55c92f5"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318c322",
   "metadata": {
    "id": "0318c322"
   },
   "source": [
    "# Catégorisez automatiquement des questions - Stack Overflow\n",
    "\n",
    "#### Objectif: \n",
    "Développement d'un système de suggestion de tags via une API pour facilité la recherche aux utilisateurs sur le site Stack Overflow. Celui-ci prendra la forme d’un algorithme de machine learning qui assignera automatiquement plusieurs tags pertinents à une question.\n",
    "\n",
    "\n",
    "Dans ce notebook:\n",
    "\n",
    "- Récupération des données sur le site StackExchange\n",
    "- Nettoyage du texte\n",
    "- Vectorisation\n",
    "\n",
    "### Sommaire\n",
    "\n",
    "* [Import](#chapter1)\n",
    "    * [Fichiers](#section_1_1)\n",
    "    * [Librairies](#section_1_2)    \n",
    "    * [Nos Données](#section_1_3)\n",
    "\n",
    "* [Text Cleaning](#chapter2)\n",
    "    * [Lower case](#section_2_1)\n",
    "    * [Noise Removal](#section_2_2) \n",
    "    * [Punctuation](#section_2_3)\n",
    "\n",
    "* [Preprocess data](#chapter3)\n",
    "    * [Tokenization](#section_3_1)\n",
    "    * [Remove stopwords](#section_3_2) \n",
    "    * [Lemmatization](#section__3)\n",
    "    \n",
    "* [Vectorization](#chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X1dcjBChtPxi",
   "metadata": {
    "id": "X1dcjBChtPxi"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270080c6",
   "metadata": {
    "id": "270080c6"
   },
   "source": [
    "## Import <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "### Import des fichiers <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcf4b9",
   "metadata": {
    "id": "b1dcf4b9"
   },
   "source": [
    "#### Récupération des données sur le site StackExchange\n",
    "\n",
    "Remarque: On peut charger au maximum 50 000 entrées/ requête SQL\n",
    "Période: 2020-01-01 à 2022-09-30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebd5b6",
   "metadata": {
    "id": "85ebd5b6"
   },
   "source": [
    "#### Code SQL utilisé\n",
    "SELECT TOP 500000 CreationDate,Title, Body, Tags, Id, Score, ViewCount, FavoriteCount, AnswerCount\n",
    "FROM Posts \n",
    "WHERE PostTypeId = 1 AND ViewCount > 10 AND FavoriteCount > 10\n",
    "AND Score > 5 AND AnswerCount > 0 AND LEN(Tags) - LEN(REPLACE(Tags, '<','')) >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b875ad",
   "metadata": {
    "id": "00b875ad"
   },
   "source": [
    "### Import des librairies <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e06590",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5e06590",
    "outputId": "dfd10aaa-c2c8-4168-fef6-070536d3d5c7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import IPython.display\n",
    "\n",
    "import seaborn as sns \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import textblob \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import requests \n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616d089",
   "metadata": {
    "id": "e616d089"
   },
   "source": [
    "#### Récuperation des fichiers de StackExchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bafd4",
   "metadata": {
    "id": "591bafd4"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YlframBTpe-S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlframBTpe-S",
    "outputId": "940f15b7-1663-432f-f59c-237a1f8dcca5"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zRCsQIXxVBcu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "zRCsQIXxVBcu",
    "outputId": "953c0516-4200-4964-f11b-2e5ee019abf2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5lv7TdfsVRQd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "5lv7TdfsVRQd",
    "outputId": "79a682ce-f499-46bd-b505-64d30ef30b27"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d87f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "670d87f9",
    "outputId": "8cec6a60-aa61-4101-a92d-333e83787c17"
   },
   "outputs": [],
   "source": [
    "data[\"Tags\"].value_counts(ascending=False).loc[lambda x : x>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CSjmt6-Pux4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CSjmt6-Pux4e",
    "outputId": "e2f8f95e-bb20-49a7-827c-0a2b8d731dd9"
   },
   "outputs": [],
   "source": [
    "data[\"Tags\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hk8W8V222Nko",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hk8W8V222Nko",
    "outputId": "19738fb4-e99c-48eb-f5dd-f78a3e50c38b"
   },
   "outputs": [],
   "source": [
    "data[\"Tags\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CH2BuMT6s1Fn",
   "metadata": {
    "id": "CH2BuMT6s1Fn"
   },
   "source": [
    "## Text Cleaning <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xYdPiVeN9r7O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYdPiVeN9r7O",
    "outputId": "be60ecc0-13a6-4986-ec85-71288af31191"
   },
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f93e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CreationDate'] = pd.to_datetime(data['CreationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regroupe les données en annuel\n",
    "year = data.groupby(pd.Grouper(key='CreationDate',\n",
    "                                    freq='1Y')).agg({'Title': 'count'})\n",
    "\n",
    "# Evolution au fur et a mesure des années\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "sns.lineplot(data=year, x=year.index, y='Title')\n",
    "plt.axhline(year.Title.mean(), \n",
    "            color=\"r\", linestyle='--',\n",
    "            label=\"Mean of question per year : {:04d}\"\\\n",
    "                   .format(int(year.Title.mean())))\n",
    "plt.xlabel(\"Date of questions\")\n",
    "plt.ylabel(\"Number of questions\")\n",
    "plt.title(\"Evolution du nombre de postes de 2009 to 2022\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Evolution du nombre de postes\",transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longeur des titres\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=data.Title.str.len())\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, end, 5))\n",
    "plt.axvline(data.Title.str.len().median() - data.Title.str.len().min(),\n",
    "            color=\"r\", linestyle='--',\n",
    "            label=\"Title Lenght median : \"+str(data.Title.str.len().median()))\n",
    "ax.set_xlabel(\"Lenght of title\")\n",
    "plt.title(\"Title lenght of Stackoverflow questions\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizer for Body characters lenght\n",
    "X = pd.DataFrame(data.Body.str.len())\n",
    "\n",
    "# Sklearn discretizer with 200 bins\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "discretizer = KBinsDiscretizer(n_bins=200,\n",
    "                               encode='ordinal',\n",
    "                               strategy='uniform')\n",
    "body_lenght = discretizer.fit_transform(X)\n",
    "body_lenght = discretizer.inverse_transform(body_lenght)\n",
    "body_lenght = pd.Series(body_lenght.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31964784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94214950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=body_lenght)\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, end, 25))\n",
    "ax.set_xlabel(\"Lenght of Body (after discretization)\")\n",
    "plt.title(\"Body lenght of Stackoverflow questions\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Bbu4Oco4_7A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "id": "5Bbu4Oco4_7A",
    "outputId": "d81040a7-82dd-46d1-fd63-7563b3258eae"
   },
   "outputs": [],
   "source": [
    "# Nous allons d'abord concatener le body et le titre\n",
    "\n",
    "data['Text'] = data['Title'] + data['Body']\n",
    "#data = data.drop(['Title','Body'], axis=1)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1964f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ba04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discretizer for Body characters lenght\n",
    "X = pd.DataFrame(data.Text.str.len())\n",
    "\n",
    "# Sklearn discretizer with 200 bins\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "discretizer = KBinsDiscretizer(n_bins=200,\n",
    "                               encode='ordinal',\n",
    "                               strategy='uniform')\n",
    "body_lenght = discretizer.fit_transform(X)\n",
    "body_lenght = discretizer.inverse_transform(body_lenght)\n",
    "body_lenght = pd.Series(body_lenght.reshape(-1))\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=body_lenght)\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, end, 25))\n",
    "ax.set_xlabel(\"Lenght of Body (after discretization)\")\n",
    "plt.title(\"Body lenght of Stackoverflow questions\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pChpTmuoAgO2",
   "metadata": {
    "id": "pChpTmuoAgO2"
   },
   "source": [
    "## Nettoyage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wFe1Bmn_oq5b",
   "metadata": {
    "id": "wFe1Bmn_oq5b"
   },
   "source": [
    "### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VLHTWCdWopCp",
   "metadata": {
    "id": "VLHTWCdWopCp"
   },
   "outputs": [],
   "source": [
    "data['Text_lower']= data['Text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove code\n",
    "def remove_code(html):\n",
    "  \n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "  \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text_BeautifulSoup']=data['Text_lower'].apply(remove_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad94f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c6e76",
   "metadata": {},
   "source": [
    "### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy6JI6lyNlU8",
   "metadata": {
    "id": "xy6JI6lyNlU8"
   },
   "outputs": [],
   "source": [
    "# Substitution de hashtags, '@', html, ponctuation\n",
    "def regex(text):\n",
    "    \n",
    "    # Remove html tags\n",
    "    text=re.sub(r\"<[^>]*>\",' ', str(text))\n",
    "    # Remove usernames \"@\"\n",
    "    text=re.sub(r'@\\S+', ' ', text)\n",
    "    # Remove hashtags\n",
    "    text=re.sub(r'#\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text=re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove unicode characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Remove unicode characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Remove English contractions\n",
    "    text = re.sub(\"\\'\\w+\", '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "    # Remove links\n",
    "    text = re.sub(r'http*\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    # Remove irrelevant characters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Remove whitespace\n",
    "    text=re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "    text=text.replace('\\n', ' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uVKBJsEXN-3P",
   "metadata": {
    "id": "uVKBJsEXN-3P"
   },
   "outputs": [],
   "source": [
    "data['Text_regex'] = data['Text_BeautifulSoup'].apply(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39103a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text_regex'][13690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tag_clean(text):\n",
    " #   text=text.replace('<','')\n",
    "  #  text=text.replace('>','')\n",
    "    \n",
    "   # return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Tags']=data['Tags'].apply(tag_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KAC0vbAM7MDR",
   "metadata": {
    "id": "KAC0vbAM7MDR"
   },
   "source": [
    "\n",
    "## Preprocessing <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "### Spelling correction <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "Nous supposons que dans le forum,blog il y a souvent les erreurs de frappe ou "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yiuL78YtEXpO",
   "metadata": {
    "id": "yiuL78YtEXpO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ijRt51tkqc9",
   "metadata": {
    "id": "7ijRt51tkqc9"
   },
   "outputs": [],
   "source": [
    "def corr(text):\n",
    "\n",
    "  text=TextBlob(text).correct()\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7xoqKSDXEb0V",
   "metadata": {
    "id": "7xoqKSDXEb0V"
   },
   "source": [
    "### Tokenization <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_6_wMWElX2o",
   "metadata": {
    "id": "1_6_wMWElX2o"
   },
   "outputs": [],
   "source": [
    "data['Text_tokenized']=data['Text_regex'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FECDSF0FlMM2",
   "metadata": {
    "id": "FECDSF0FlMM2"
   },
   "source": [
    "### Stopwords <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a00b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_stopwords_func(text):\n",
    "    '''\n",
    "    Removes Stop Words (also capitalized) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without Stop Words\n",
    "    ''' \n",
    "    # check in lowercase \n",
    "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mszp9axkFRIk",
   "metadata": {
    "id": "mszp9axkFRIk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "data['Text_stopwords'] = data['Text_tokenized'].apply(lambda text: [token for token in text if token not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4IL03WvMHGD6",
   "metadata": {
    "id": "4IL03WvMHGD6"
   },
   "source": [
    "### Lemmatization <a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tXqgF0_8HQfL",
   "metadata": {
    "id": "tXqgF0_8HQfL"
   },
   "outputs": [],
   "source": [
    "def get_pos_tags(word):\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, #adjective\n",
    "                \"N\": wordnet.NOUN,#noun\n",
    "                \"V\": wordnet.VERB,#verb\n",
    "                \"R\": wordnet.ADV} #adverb\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemma_fct(list_words) :\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w, get_pos_tags(w)) for w in list_words]\n",
    "    return lem_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text_lemmatized']=data['Text_stopwords'].apply(lemma_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char_func(text, threshold=1):\n",
    "    threshold = threshold\n",
    "    words = word_tokenize(text)\n",
    "    text = ' '.join([w for w in words if len(w) > threshold])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_text'] = data['Text_lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f39056",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_text']= data['Cleaned_text'].apply(remove_single_char_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbd11b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate lenght of each list in Body\n",
    "#data['body_tokens_count'] = [len(_) for _ in data.Cleaned_text]\n",
    "data['body_tokens_count'] = data['length_cleaned_text'] \n",
    "# Countplot of body lenght\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=data.body_tokens_count)\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, end, 25))\n",
    "plot_median = data.body_tokens_count.median()\n",
    "plt.axvline(plot_median - data.body_tokens_count.min(),\n",
    "            color=\"r\", linestyle='--',\n",
    "            label=\"Body tokens Lenght median : \"+str(plot_median))\n",
    "ax.set_xlabel(\"Lenght of body tokens\")\n",
    "plt.title(\"Body tokens lenght of Stackoverflow questions after cleaning\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.body_tokens_count.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff6309",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.body_tokens_count.quantile([0.25,0.5,0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.body_tokens_count.quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4976a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.body_tokens_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c778b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a01db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data.body_tokens_count > data.body_tokens_count.quantile(0.95)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aca1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length_cleaned_text'] = data['Cleaned_text'].apply(lambda x : len(word_tokenize(str(x))))\n",
    "max_length_cleaned_data = data['length_cleaned_text'].max()\n",
    "print(\"max length Cleaned_text : \", max_length_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57422eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data on body lenght\n",
    "data = data[data.Body.str.len() < 4000]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_text'] = data['Cleaned_text'].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_text'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3a44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['length_cleaned_text']==3090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a883a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['Cleaned_text'][13690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words\n",
    "def word_count_func(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52585fc0",
   "metadata": {},
   "source": [
    "### Les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef3882",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"Tags\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c13788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace open and close balise between tags\n",
    "data['Tags'] = data['Tags'].str.translate(str.maketrans({'<': '', '>': ','}))\n",
    "\n",
    "# Delete last \",\" for each row\n",
    "data['Tags'] = data['Tags'].str[:-1]\n",
    "data['Tags'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf99dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_split_tags(df, column, separator):\n",
    "    \"\"\"This function allows you to split the different words contained\n",
    "    in a Pandas Series cell and to inject them separately into a list.\n",
    "    This makes it possible, for example, to count the occurrences of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    df : Pandas Dataframe\n",
    "        Dataframe to use.\n",
    "    column : string\n",
    "        Column of the dataframe to use\n",
    "    separator : string\n",
    "        Separator character for str.split.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    list_words = []\n",
    "    for word in df[column].str.split(separator):\n",
    "        list_words.extend(word)\n",
    "    df_list_words = pd.DataFrame(list_words, columns=[\"Tag\"])\n",
    "    df_list_words = df_list_words.groupby(\"Tag\")\\\n",
    "        .agg(tag_count=pd.NamedAgg(column=\"Tag\", aggfunc=\"count\"))\n",
    "    df_list_words.sort_values(\"tag_count\", ascending=False, inplace=True)\n",
    "    return df_list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d36279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = count_split_tags(df=data, column='Tags', separator=',')\n",
    "print(\"Le jeu de données compte {} tags.\".format(tags_list.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of splits\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=tags_list.iloc[0:40, :],\n",
    "            x=tags_list.iloc[0:40, :].index,\n",
    "            y=\"tag_count\", color=\"#f48023\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"40 most popular tags in Stackoverflow (2009 - 2020)\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee216e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot word cloud with tags_list (frequencies)\n",
    "fig = plt.figure(1, figsize=(17, 12))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "wordcloud = WordCloud(width=900, height=500,\n",
    "                      background_color=\"black\",\n",
    "                      max_words=500, relative_scaling=1,\n",
    "                      normalize_plurals=False)\\\n",
    "    .generate_from_frequencies(tags_list.to_dict()['tag_count'])\n",
    "\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"Word Cloud of 500 best Tags on StackOverflow (2009 - 2020)\\n\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103e224",
   "metadata": {},
   "source": [
    "Il est intéressant de voir si les tags populaires ont évolué au cours du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d12ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots parameters\n",
    "years = {0: 2009, 1: 2012, 2: 2018, 3: 2022}\n",
    "colors = {0: \"#ff5858\", 1: \"#dba0db\",\n",
    "          2: \"#72d9f0\", 3: \"#a5cf27\"}\n",
    "subplots = 4\n",
    "cols = 2\n",
    "rows = subplots // cols\n",
    "rows += subplots % cols\n",
    "position = range(1, subplots + 1)\n",
    "\n",
    "# Plot popular tags for each year\n",
    "fig = plt.figure(1, figsize=(20, 16))\n",
    "for k in range(subplots):\n",
    "    subset = data[data[\"CreationDate\"].dt.year == years[k]]\n",
    "    temp_list = count_split_tags(df=subset, column='Tags', separator=',')\n",
    "    ax = fig.add_subplot(rows, cols, position[k])\n",
    "    sns.barplot(data=temp_list.iloc[0:20, :],\n",
    "            x=temp_list.iloc[0:20, :].index,\n",
    "            y=\"tag_count\", color=colors[k])\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_title(\"20 most popular tags for {}\".format(years[k]),\n",
    "                 fontsize=18, color=\"#fcc642\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa386ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de Tags par question :\n",
    "\n",
    "# Create a list of Tags and count the number\n",
    "data['Tags_list'] = data['Tags'].str.split(',')\n",
    "data['Tags_count'] = data['Tags_list'].apply(lambda x: len(x))\n",
    "\n",
    "# Plot the result\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = sns.countplot(x=data.Tags_count, color=\"#f48023\")\n",
    "ax.set_xlabel(\"Tags\")\n",
    "plt.title(\"Number of tags used per question\",\n",
    "          fontsize=18, color=\"#fcc642\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6beb3",
   "metadata": {},
   "source": [
    "Comme nous avons plus de 27 000 tags et que les algorithmes NLP sont assez lents, nous allons filtrer nos données. Nous allons prendre seulement les sujets qui sont dans les 50 meilleurs tags et supprimer le reste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tag(x, top_list):\n",
    "    \"\"\"Comparison of the elements of 2 lists to \n",
    "    check if all the tags are found in a list of top tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    x : list\n",
    "        List of tags to test.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    for item in x:\n",
    "        if (item in top_list):\n",
    "            #x.remove(item)\n",
    "            temp_list.append(item)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e0457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_tags = list(tags_list.iloc[0:50].index)\n",
    "data['Tags_list'] = data['Tags_list']\\\n",
    "                    .apply(lambda x: filter_tag(x, top_tags))\n",
    "data['number_of_tags'] = data['Tags_list'].apply(lambda x : len(x))\n",
    "data = data[data.number_of_tags > 0]\n",
    "print(\"New size of dataset : {} questions.\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5267201",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_clean(text):\n",
    "    text=text.replace('[','')\n",
    "    text=text.replace(']','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['top_tags'] =data['Tags_list'].apply(lambda x:', '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2882e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cleaned_text'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['CreationDate',\n",
    "                  'Title',\n",
    "                  'Id',\n",
    "                  'Score',\n",
    "                  'ViewCount',\n",
    "                  'FavoriteCount',\n",
    "                  'AnswerCount',\n",
    "                  'Text_lower',\n",
    "                  'Text_BeautifulSoup',\n",
    "                  'Text_regex',\n",
    "                 'Text_tokenized',\n",
    "                 'Text_stopwords',\n",
    "                 'Text_lemmatized',\n",
    "                 'Tags_list',\n",
    "                  'Tags_count',\n",
    "                  'number_of_tags'], \n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82266f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "data.to_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153ce2e",
   "metadata": {},
   "source": [
    "### Wordcloud vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56caed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new wordcloud.\n",
    "wordcloud_precleaning = WordCloud(random_state = 8,\n",
    "        normalize_plurals = False,\n",
    "        width = 600, height= 300,\n",
    "        max_words = 300,\n",
    "        stopwords = [])\n",
    "\n",
    "text_brut = ' '.join(data['Text'])\n",
    "# Apply the wordcloud to the text.\n",
    "wordcloud_precleaning.generate(text_brut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new wordcloud.\n",
    "wordcloud_cleaned_text = WordCloud(random_state = 8,\n",
    "        normalize_plurals = False,\n",
    "        width = 600, height= 300,\n",
    "        max_words = 300,\n",
    "        stopwords = [])\n",
    "\n",
    "text_cleaned = ' '.join(data['Cleaned_text'])\n",
    "# Apply the wordcloud to the text.\n",
    "wordcloud_cleaned_text.generate(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efaee6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig, ax = plt.subplots(1,2, figsize = (20,10))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(wordcloud_precleaning, interpolation='bilinear')\n",
    "plt.title(\"Avant nettoyage\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(wordcloud_cleaned_text, interpolation='bilinear')\n",
    "plt.title(\"Aprés nettoyage\",)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_clean(text):\n",
    "    text=text.replace('<','')\n",
    "    text=text.replace('>',' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e38d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_tag=WordCloud()\n",
    "tags=' '.join(data['Tags'].apply(tag_clean))\n",
    "\n",
    "wordcloud_tag.generate(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud_tag, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aIVVllFKtTlS",
   "metadata": {
    "id": "aIVVllFKtTlS"
   },
   "source": [
    "##  Vectorization <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZlC-ypN0i9z",
   "metadata": {
    "id": "mZlC-ypN0i9z"
   },
   "outputs": [],
   "source": [
    "data['Text_toVectorize'] = data['Cleaned_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KqNw9FNE0w9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "KqNw9FNE0w9W",
    "outputId": "57b9d56c-b40f-4aa5-a1f1-67991f5dd403"
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vA2HJ6YjtXHU",
   "metadata": {
    "id": "vA2HJ6YjtXHU"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(data['Text_toVectorize'])\n",
    "\n",
    "X.todense()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
